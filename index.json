[{"authors":[],"categories":["Data Wrangling","Prediction Models","DBMS","SQL"],"content":"This project was about working with machine learning models and predictions while sourcing from an online cloud object. IN THE MIDDLE OF THE HOLIDAYS. Talk about stress!\nThis is how we did it:\nAfter establishing access on the cloud system for all persons working on the project, we create connnection objects within the RStudio project session and save within our local environment. Unless your team is set up with Version Control, each individual will have to create the connection object within their own workspace and global environment. Install or load:\n install.packages(\u0026quot;DBI\u0026quot;) install.packages(\u0026quot;bigrquery\u0026quot;)   These packages allow R to interact with Google\u0026rsquo;s BigQuery (which is an online Database Management System) smoothly. Once installed and loaded you can begin setting up your local connection.\nCreate a new .R or .r script and you are able to save the con /(connection) object within the local environment. The code should look similar too:\n# function within DBI package con = dbConnect( # specifies which DBMS bigquery(), project = \u0026quot;your_proj_here\u0026quot;, dataset = \u0026quot;your_dataset_here\u0026quot; )  Once saved into your local and global environment, put the .r/.R script into the .gitignore file. Make sure everyone on the team does this or your data will be accessed by all (remember: version control)! While we are setting up connections run this line of code through your console\noptions(httr_oob_default = TRUE)\nthis allows a simple bypass to access APIs through R. (Technically the con is this, BUUUT R is fickle.)\nAnnoying Reminder: Run this code everytime R loads | set add into preloaded code chunks that run when you automaticaly start a session. There\u0026rsquo;s a lot of blogs about how to create preloads if you need guidance. Sad to say, this isn\u0026rsquo;t one\u0026hellip;moving onward!\nThere\u0026rsquo;s going to be a message that pops up in the console similar to this:\n` Use a local file ('.httr-oauth'), to cache OAuth access credentials between R sessions? # 1: Yes # 2: No`  Depending on how secure you want your connection to be will determine which option you choose. More or less:\n1 = Local file is created but whoever has access to your computer will have access to the file. 2 = No file is created and you are brought through a secondary verification process. Copy access code from new tab into console.  This particular client had pretty sweet info so we choose to go with the second option along with other security measures. If you are working with R or RStudio, you\u0026rsquo;ve noticed memory capacities and other slightly annoying errors encountered with working with large datasets. If you\u0026rsquo;re reading this, you know exactly what I\u0026rsquo;m referring too:\n Can\u0026rsquo;t pull DataSets/Tables with over 100k rows without crashing R. Memory issues where the app just crashes (doesn\u0026rsquo;t even give an actual error code sometimes; I\u0026rsquo;m still locked out of one of my R sessions.) etc etc.   FORTUNATELY enough for you BigQuery helps R by doing a \u0026ldquo;LazyQuery\u0026rdquo; where BigQuery translates R into SQL, applies filters/functions you coded to a small section of the datatable and then prints a 10 x ?? tibble for you.\nAlthough this may feel unsecure, the small tibbles allow you to see IF your filters/functions were applied properly!\nThis is also where the dplyr::count() and dplyr::collect() calls come in clutch. dplyr::count() acts as a a wrapper with group_by and ungroup. This allows all things to be accounted for according to the respective filters. dplyr::collect() allows local tibbles to be created.\nEx:\n# count() groups the variables within specified column as factors without having to use as.factor(). After computation has been printed count() automaticaly ungroups. mtcars %\u0026gt;% count(cyl) # A tibble: 3 x 2 # cyl n # \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; # 1 4 11 # 2 6 7 # 3 8 14 # collect allows the table to be created into the local environment by pulling and retrieving data from the SQL query on the DBMS. # if this was only called without collect the R session would crash because R will pull entire dataset. mtcars %\u0026gt;% select(gear, cyl) %\u0026gt;% group_by(gear) %\u0026gt;% summarise(cyl_amount = n_distinct(cyl)) %\u0026gt;% collect() # A tibble: 3 x 2 # gear cyl_amount # \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; # 1 3 3 # 2 4 2 # 3 5 3  These two useful functions will allow you to pull and extract data without crashing your session! Just be careful what you collect!\nBecause our client\u0026rsquo;s dataset was so untidy it was extremely stressful tidying!!! BigQuery/SQL saves the certain numeric classes into FLOAT numbers, which can be tricky to integrate into R.\nR KNOWS these numbers to correlate as Double-Precision Vectors. It\u0026rsquo;s super frustating because it\u0026rsquo;s the same thing BUT R just doesn\u0026rsquo;t recognize it as such. :(\nI only bring this up if you have to do calculations with your original FLOAT variables. It\u0026rsquo;s a slight pain to convert, re-convert etc etc. To help alleviate some SCRESS load these packages into your sesh:\nlibrary(devtools) library(dplyr) library(bigrquery) # quick promo - shoutout github::@kbmorales library(PFbaseproject) library(DBI) library(odbc) library(here) library(stats) library(dbplot) library(dbplyr) library(stringr) # library(scater) library(broom)  With these loaded most FLOAT values should play within R nicely with other NUMERIC or INTEGER variables, but no promises. (._. )\nSEARCHING THROUGH LARGE DATABASES\nWoohoooooooooooooooo\u0026hellip;.no?\nWELL, learning some regular expression will DEFINITELY help out when finding specifics in R. Especially filtering and sifting through character string variables and you are unsure about what the data hides. For perspective we were working with a DBMS, with 4 Datatables and each table had 3mil+ rows. This may seem normal to most experts, but for a newbie like myself it was overwhelming for sure!!!\nThere are a few hiccups when working with R and a SQL query. There are certin base R functions that are not recognized within SQL. If you know SQL code working around this shouldn\u0026rsquo;t be hard, if not it\u0026rsquo;s a struggle to work around.\nThe functions we specifically used in conjunction with regular expressions were stringr::str_detect indexed within the column we are inquiring about. Code should look similar to this:\n# using zipcodes as an example zip \u0026lt;- con %\u0026gt;% tbl(\u0026quot;query_table\u0026quot;) %\u0026gt;% select(zipcodes) %\u0026gt;% # when count() \u0026amp; collect() are called without an expression or variable listed inside # the default is to apply the function to all the code above it # only if piped! count() %\u0026gt;% collect() # tibble we created because of the collect() call # lol zip \u0026lt;- zip[str_detect(zip$zipcode, \u0026quot;\\\\d{5}-\u0026quot;)]  The above line of code is detecting all numeric strings within the tibble zip and column zipcode, \u0026quot;\\\\d\u0026quot; that include exactly five digits {5} and a dash -.\nBecause we set zip as a tibble with count() AND collect() when we call the string detected line of code a tibble will return with the different zipcodes grouped and meeting the requirements listed. Here is an example below:\n# # A tibble: 13 x 2 # # Groups: zipcode [13] # zipcode n # \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; # 1 13455-5589 8 # 2 10452-8001 46 # 3 10467-2410 37 # 4 76661- 13 # 5 09394- 919 # 6 79079- 456 # 7 92834- 385 # 8 15235-5505 44627 # 9 19013-3840 420 # 10 19401-4715 1422 # 11 77640- 3399 # 12 59860-7037 10 # 13 NA NA  Notice that the line of code also returned some numeric strings that had numbers following the dash. That is because in our regular expression there was not an $ anchor at the end of the expression to tell R that we only want 5 digit zips WITHOUT more numbers. This is something to keep in mind!\nStill working with the package stringr we were able to truncate strings and split them based on our needs via the client\u0026rsquo;s wants. We ended up using stringr::str_truncate() for the zipcodes, as it was easier and provided cleaner code production. There was an issue we had where one column held a lot of information that could be split up for easier wrangling. stringr::str_split() definitely helped! It was a column full of alphabetic and numeric character strings that needed to be separated.\nHere\u0026rsquo;s some example reproducible code provided by one of our team leads @jtleek !\n# create these extra columns and repeat exactly however many # empty rows needed within the DT, tibble, DF etc etc. newcol1 = newcol2 = newcol3 = rep(NA,dim(dataframe)[1]) # conditional statement that allows us to detect # a string and split the string into sections for(i in 1:dim(dataframe)[1]){ # created function that tells R to look for a \u0026quot;space\u0026quot; # within the name tmp = str_split(dataframe$originalcol[i],\u0026quot;[ ]{1,}\u0026quot;) # if there are 3 spaces within the string = 1 space if(length(tmp[[1]])==3){ newcol1[i] = tmp[[1]][1] newcol2[i] = tmp[[1]][2] newcol3[i] = tmp[[1]][3] } # if four spaces do the same if(length(tmp[[1]])==4){ newcol1[i] = tmp[[1]][1] newcol2[i] = paste(tmp[[1]][2:3],collapse=\u0026quot;\u0026quot;) newcol3[i] = tmp[[1]][4] } } # renaming and setting columns dataframe$newcol1 = newcol1 dataframe$newcol2 = newcol2 dataframe$newcol3 = newcol3  Also SQL has problems reading large nested code chunks. It will get confused with parenthesises (another random issue). BigQuery SQL WILL NOT read queries (r code chunks) longer than 1024.00k characters. I ran into this problem a couple of times running a large filtered code chunk I created. Creating functions and source files will help DRAMATICALLY as far as saving ram/memory, cpu, and preventing the error code for long queries. An awesome hot tip to consider!\nLet\u0026rsquo;s talk about workflow\u0026hellip; . Working under different management can be confusing and sometimes challenging. Luckily I am apart of a pretty amazing team, although I recommend that if you\u0026rsquo;re working with a database THAT extensive: try not make the deadline centered around the holidays! Especially if the holiday takes up most of your work week. The team found themselves exhausted and exasporated because of the short amount of time we had to figure this stuff out!\nPlease give yourself an adequate amount of time to analyze, improve, test and restructure the project/task at hand!\n","date":1547510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547510400,"objectID":"3ea2574ea2119f6951a6e62ae294a42b","permalink":"/post/lessons-learned-with/","publishdate":"2019-01-15T00:00:00Z","relpermalink":"/post/lessons-learned-with/","section":"post","summary":"This project was about working with machine learning models and predictions while sourcing from an online cloud object. IN THE MIDDLE OF THE HOLIDAYS. Talk about stress!\nThis is how we did it:\nAfter establishing access on the cloud system for all persons working on the project, we create connnection objects within the RStudio project session and save within our local environment. Unless your team is set up with Version Control, each individual will have to create the connection object within their own workspace and global environment.","tags":["SQL","DBMS","BigQuery","R","Entry Level"],"title":"Lessons Learned - First Client Project","type":"post"},{"authors":null,"categories":["R","survey data","data analysis"],"content":" Intro \u0026amp; Background This survey was created to test our cohort’s strengths in data visualization, data wrangling and data tidying. The American Time Use Survey is a public survey which enlists how Americans use their time in liesure, work and extra curricular activities. This data can be used in a variety of cross analysis’ and formats for companies within and outside of the United States.\n Data The American Time Use Survey (ATUS) is a time-use survey of Americans, which is sponsored by the Bureau of Labor Statistics (BLS) and conducted by the U.S. Census Bureau. Respondents of the survey are asked to keep a diary for one day carefully recording the amount of time they spend on various activities including working, leisure, childcare, and household activities. The survey has been conducted every year since 2003.\nYou can find the data dictionaries for each year on https://www.bls.gov/tus/dictionaries.htm\n#load data from data files onto the workspace atus.cps \u0026lt;- read.delim(\u0026#39;/cloud/project/website/content/project/atus_survey/raw_data/atuscps_2016.dat\u0026#39;, sep=\u0026quot;,\u0026quot;) atus.sum \u0026lt;- read.delim(\u0026#39;/cloud/project/website/content/project/atus_survey/raw_data/atussum_2016.dat\u0026#39;, sep=\u0026quot;,\u0026quot;) # joining both files together by respondents\u0026#39; ID atus.all \u0026lt;- atus.sum %\u0026gt;% left_join(atus.cps %\u0026gt;% filter(TULINENO==1), by = c(\u0026quot;TUCASEID\u0026quot;))  Exploratory Data Analysis ## write out al columns up to t030112 for sum of CHILDCARE column. atus.all \u0026lt;- atus.all %\u0026gt;% mutate(CHILDCARE = t030101 + t030102 + t030103 + t030104 + t030105 + t030106 + t030108 + t030109 + t030110 + t030111 + t030112 %\u0026gt;% glimpse(CHILDCARE)) #exclude all missing factors ## replace -1 in the variable TRDPFTPT with NA. atus.all$TRDPFTPT[atus.all$TRDPFTPT == -1] \u0026lt;- NA %\u0026gt;% sum(is.na(atus.all$TRDPFTPT))##take this away, filter once done. grep(\u0026quot;TRHHCHILD\u0026quot;,names(atus.all)) ## integer(0) #find the amount of missing values in the column sum(is.na(atus.all$TRDPFTPT)) ## [1] 4119 class(atus.all$TRYHHCHILD) ## [1] \u0026quot;integer\u0026quot; We need to create a universal table so that we can continue to wrangle the data efficiently; in the second code chunk I am removing unecessary, or missing, data.\n Analysis Approach In this analysis we are choosing to hone in on how respondents’ spend time with their children. I am interested in family and home interactions and the different dynamics that in turn either allow or revoke certain interactions based on each respondents’ demographics. There have not been many analysis’ on how people spend time with their children. Most findings are not as unusual.\n Results We asked different questions to work through some of the data. The results are as follows:\nResults 1 Do younger people spend more time with their children than the older generation?\n## do younger people spend more time with children than older people. adults_atLeast_one_kid \u0026lt;- atus.all %\u0026gt;% select(CHILDCARE, TEAGE, TRYHHCHILD, HEFAMINC, TRCHILDNUM, PEMARITL, TRDPFTPT, TESEX) %\u0026gt;% filter(TRCHILDNUM \u0026gt; 0) ggplot(adults_atLeast_one_kid, aes(x = TEAGE, y = CHILDCARE)) + geom_point(aes(color = factor(TEAGE)), size = 0.5) + theme(legend.position = \u0026quot;none\u0026quot;) + labs( x = \u0026quot;RESPONDENT\u0026#39;S AGE \\n years\u0026quot;, y = \u0026quot;CHILDCARE \\n minutes per week\u0026quot;, title = \u0026quot;Do younger people spend more time with \\n their children than older people?\u0026quot;) Looking at the graph one can argue that younger adults spend more time with children than the older generation. * Since our data set are respondents between the ages 17 to around 85, our dividing(median) age is 50. By looking at the chart you can see that individuals below 50 spend more time with their children than those older than them. It is also colored by respondents’ age.\n** Note that this data only includes respondents with at least one child within the household that they take care of. And this variable will stay constant for the next three graphs!** ## Results 2\n## do affluent people spend more time with children than non-affluent ggplot(adults_atLeast_one_kid, aes(x = HEFAMINC, y = CHILDCARE, col = TESEX )) + geom_point(aes(color = factor(TEAGE), ), size = 1) + theme(legend.position = \u0026quot;none\u0026quot;) + labs( x = \u0026quot;FAMILY INCOME \\n tens of thousands\u0026quot;, y = \u0026quot;CHILDCARE \\n minutes per week\u0026quot;, title = \u0026quot;Do affluent respondents spend more time with \\n their children than non-affluent respondents?\u0026quot;) In this graph we can see that a respondent’s income* does not necessarily affect the amount of time spent with their respective child, other than the obvious outliers within each tax bracket. Although you can see a slight rise, or peak, after the 12th tax bracket! The colors are by age.\n* Income key: + 1 Less than $5,000 + 2 $5,000 to $7,499 + 3 $7,500 to $9,999 + 4 $10,000 to $12,499 + 5 $12,500 to $14,999 + 6 $15,000 to $19,999 + 7 $20,000 to $24,999 + 8 $25,000 to $29,999 + 9 $30,000 to $34,999 + 10 $35,000 to $39,999 + 11 $40,000 to $49,999 + 12 $50,000 to $59,999 + 13 $60,000 to $74,999 + 14 $75,000 to $99,999 + 15 $100,000 to $149,999 + 16 $150,000 and over *Note that income is based on household, not individual respondent.*\n Results 3 ## select atus.wide \u0026lt;- atus.all %\u0026gt;% mutate(act01 = rowSums(atus.all[,grep(\u0026quot;t01\u0026quot;, names(atus.all))]), act02 = rowSums(atus.all[,grep(\u0026quot;t02\u0026quot;, names(atus.all))]), act03 = rowSums(atus.all[,grep(\u0026quot;t03\u0026quot;, names(atus.all))]), act04 = rowSums(atus.all[,grep(\u0026quot;t04\u0026quot;, names(atus.all))]), act05 = rowSums(atus.all[,grep(\u0026quot;t05\u0026quot;, names(atus.all))]), act06 = rowSums(atus.all[,grep(\u0026quot;t06\u0026quot;, names(atus.all))]), act07 = rowSums(atus.all[,grep(\u0026quot;t07\u0026quot;, names(atus.all))]), act08 = rowSums(atus.all[,grep(\u0026quot;t08\u0026quot;, names(atus.all))]), act09 = rowSums(atus.all[,grep(\u0026quot;t09\u0026quot;, names(atus.all))]), act10 = rowSums(atus.all[,grep(\u0026quot;t10\u0026quot;, names(atus.all))]), act11 = rowSums(atus.all[,grep(\u0026quot;t11\u0026quot;, names(atus.all))]), act12 = rowSums(atus.all[,grep(\u0026quot;t12\u0026quot;, names(atus.all))]), act13 = rowSums(atus.all[,grep(\u0026quot;t13\u0026quot;, names(atus.all))]), act14 = rowSums(atus.all[,grep(\u0026quot;t14\u0026quot;, names(atus.all))]), act15 = rowSums(atus.all[,grep(\u0026quot;t15\u0026quot;, names(atus.all))]), act16 = rowSums(atus.all[,grep(\u0026quot;t16\u0026quot;, names(atus.all))]), # act17 = , there is no category 17 in the data act18 = rowSums(atus.all[,grep(\u0026quot;t18\u0026quot;, names(atus.all))])) %\u0026gt;% select(TUCASEID, TEAGE, HEFAMINC, starts_with(\u0026quot;act\u0026quot;)) %\u0026gt;% rename(\u0026quot;Traveling\u0026quot;= act18, \u0026quot;Telephone Calls and Use\u0026quot; = act16, \u0026quot;Volunteer Activities\u0026quot; = act15, \u0026quot;Religious Activities\u0026quot; = act14, \u0026quot;Exercise and Recreation Activities\u0026quot; = act13, \u0026quot;Leisure Activities\u0026quot; = act12, \u0026quot;Eating or Drinking\u0026quot; = act11, \u0026quot;Civil Obligations\u0026quot; = act10, \u0026quot;Household Services\u0026quot; = act09, \u0026quot;Professional Personal Care Services\u0026quot; = act08, \u0026quot;Consumer Purchases\u0026quot; = act07, \u0026quot;Education\u0026quot; = act06, \u0026quot;Work Related Activities\u0026quot; = act05, \u0026quot;Caring for NHH Members\u0026quot; = act04, \u0026quot;Caring for HH Members\u0026quot; = act03, \u0026quot;Houshold Activities\u0026quot; = act02, \u0026quot;Personal Care\u0026quot; = act01) head(atus.wide) df.long \u0026lt;- atus.wide %\u0026gt;% # use code to convert the wide format to long. gather(ACTIVITY, MINS, `Personal Care`:Traveling) head(df.long) ## TUCASEID TEAGE HEFAMINC ACTIVITY MINS ## 1 2.01601e+13 62 3 Personal Care 715 ## 2 2.01601e+13 69 6 Personal Care 620 ## 3 2.01601e+13 24 4 Personal Care 1060 ## 4 2.01601e+13 31 8 Personal Care 655 ## 5 2.01601e+13 59 13 Personal Care 580 ## 6 2.01601e+13 16 5 Personal Care 620 # Plot for average amount of time spent by age! # pull from the desired data set, in this case we are working with df.long df.long %\u0026gt;% # follow the instructions and grouby() Activity and age group_by(ACTIVITY, TEAGE) %\u0026gt;% # calculate the average amount of time people spend on each activity summarise(AVGMINS = mean(MINS)) %\u0026gt;% # Plot the graph! ggplot(aes(x = TEAGE, y = AVGMINS)) + geom_point() + geom_smooth(method = \u0026quot;loess\u0026quot;) + facet_wrap(~ ACTIVITY) + labs( title = \u0026quot;Average amount of time spent \\n per person\u0026#39;s age\u0026quot;) + theme(text = element_text(size=18), axis.text.x = element_text(angle=90, hjust=1)) We were asked to plot the respondent’s age against the average amount of time for each activity provided! We are asked the question : Which categories does the average time spent vary by age?\nIn this plot we can see that activities Personal Care, Work Related Activities, and Leisure Activities, vary by age!\n Results 4 activity_by_income \u0026lt;-df.long %\u0026gt;% group_by(ACTIVITY, HEFAMINC) %\u0026gt;% ## add the rest of the code here summarise(AVGMINS_WRK = mean(MINS)) %\u0026gt;% #create new colummn to set y-axis equal from 0 to 1 for proportions mutate(SumMins = sum(AVGMINS_WRK)) %\u0026gt;% #divide the new column by the average to create percentafe proportions mutate(AvgSumMins = AVGMINS_WRK/SumMins)%\u0026gt;% #plot the graph ggplot(aes(x = ACTIVITY,y = AvgSumMins, fill = factor(HEFAMINC))) + geom_bar(stat = \u0026quot;identity\u0026quot;) + # scale_fill_brewer(palette = \u0026quot;Paired\u0026quot;) + scale_fill_hue(h = c(150, 390)) + # color_ramp_palette(c(\u0026quot;red\u0026quot;, \u0026quot;yellow\u0026quot;)) + coord_flip() + labs(title = \u0026quot;Amount of time spent on activities \\n by income\u0026quot;) activity_by_income In this graph I was trying to imitate what Henrik Lindberg did in his analysis of Income distributions in America’s pastimes https://raw.githubusercontent.com/halhen/viz-pub/master/pastime-income/pastime.png. I am able to properly graph, but I am unable to get the proportions correctly!\n  ","date":1538438400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538438400,"objectID":"ab15bb999477359ae8df581b8c27b7aa","permalink":"/project/atus-survey-data/","publishdate":"2018-10-02T00:00:00Z","relpermalink":"/project/atus-survey-data/","section":"project","summary":"Analysis of 2016 American Time Use Survey (ATUS) Data","tags":["R","dataviz","analysis"],"title":"Atus Time Use Survey Analysis","type":"project"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536451200,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":null,"categories":["R","Data Wrangling","Data Tidying"],"content":" This blog post is to showcase earlier stages of learning data wrangling and tidying with R and RStudio.\nData Set 1: Sales from the Retail Trade and Food Services Report from the US Census. This dataset only covers Department Stores, though the report covers a wide range of retail types. [1992-2016]\nData Set 2 US Retail Sales by Store Type with Growth Rate [2009-2014]\n#1992-2016 #https://data.world/retail/department-store-sales GET(\u0026quot;https://query.data.world/s/gdk7iwtlisq6vkktmybqqr7hjjty5s\u0026quot;, write_disk(tf \u0026lt;- tempfile(fileext = \u0026quot;.xls\u0026quot;))) df1 \u0026lt;- read_excel(tf) #2009-2014 # https://data.world/garyhoov/retail-sales-growth GET(\u0026quot;https://query.data.world/s/py7kinxvyuxjpzwdjs2ti4wdmui6bi\u0026quot;, write_disk(tf \u0026lt;- tempfile(fileext = \u0026quot;.xls\u0026quot;))) df2 \u0026lt;- read_excel(tf) ## the the first row and make that the column names of the data frame colnames(df2) \u0026lt;- df2[1,] ## use saveRDS() to save each object as a .rds file saveRDS(df1, file = \u0026quot;df1.rds\u0026quot;) saveRDS(df2, file = \u0026quot;df2.rds\u0026quot;) df_department.rds \u0026lt;- readRDS(\u0026quot;df1.rds\u0026quot;) df_retail.rds \u0026lt;- readRDS(\u0026quot;df2.rds\u0026quot;) ## an example working with df2 ## let\u0026#39;s wrangle! df_retail \u0026lt;- df2 %\u0026gt;% ## remove the r from the column names of df2 magrittr::set_colnames(gsub(\u0026quot;r\u0026quot;,\u0026quot;\u0026quot;,df2[1,])) %\u0026gt;% ## add a new column called \u0026quot;business\u0026quot; mutate(business = gsub(\u0026quot;[…]|[.]\u0026quot;,\u0026quot;\u0026quot;,`Kind of business`)) %\u0026gt;% ## filter to include Retail sales or Department stores sales filter(grepl(\u0026#39;Retail sales, total |Department stores\u0026#39;, business)) %\u0026gt;% ## only look at columns with year information in them select(.,c(matches(\u0026#39;19|20\u0026#39;),business)) %\u0026gt;% ## take year column and collapse them into a single column gather(., \u0026quot;year\u0026quot;, \u0026quot;n\u0026quot;, 1:(ncol(.)-1)) %\u0026gt;% ## make sure the count column `n` is numeric mutate(n=as.numeric(n)) %\u0026gt;% ## filter to only include the businesses we\u0026#39;re interested in filter(business == \u0026quot;Retail sales, total \u0026quot;| business==\u0026quot;Department stores \u0026quot;) In this example code chunk we are manipulating the data to create a new column and a dataset. The current code will have added a new ‘business’ column and then manipulated the data do that only Retail and Department stores are showing.\n## work with df1 df_department \u0026lt;- df1 %\u0026gt;% ## split Period column into one column called \u0026quot;month\u0026quot; and one called \u0026quot;year\u0026quot; separate(Period, into = c(\u0026#39;month\u0026#39;,\u0026#39;year\u0026#39;), extra = \u0026#39;drop\u0026#39;, remove = FALSE) %\u0026gt;% ## add a column `value` which contains the ## information from the `Value (in millions)` mutate(value = `Value (in millions)`) %\u0026gt;% ## group the data frame by the `year` column group_by(year) %\u0026gt;% ## Summarize the data by creating a new column ## call this column `n` ## have it contain the sum of the `value` column summarize(n = sum(value)) %\u0026gt;% ### create a new column called `business` ## set the value of this column to be \u0026quot;department stores\u0026quot; ## for the entire data set mutate(business = \u0026quot;department stores\u0026quot;) %\u0026gt;% ## reorder column names select(business, year, n) This code chunk has added the new columns: value and business.\n## Now, combine the two data frames df_total \u0026lt;- bind_rows(df_department, df_retail) ## Plot Retail Sales data ggplot(df_retail, aes(x=year,y=n,colour=business)) + geom_point()  ## Plot Department Sales data ggplot(df_department, aes(x=year,y=n)) + geom_point()  ## Plot Combined Data ggplot(df_total, aes(x=year,y= as.numeric(n), colour=business)) + geom_point() ","date":1528848000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528848000,"objectID":"3ba34e93ef1e835017dbac6fbfc1e4d8","permalink":"/project/wrangling-data-department-stores/","publishdate":"2018-06-13T00:00:00Z","relpermalink":"/project/wrangling-data-department-stores/","section":"project","summary":"Basic Data Wrangling and Tidying","tags":["analysis","dataviz","R","Welcome","Entry Level"],"title":"Wrangling Data - Department Stores","type":"project"},{"authors":[],"categories":null,"content":"Click on the Slides button above to view the built-in slides feature.\n Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using url_slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"cd6d9d084287506b4668ad90c6aff50a","permalink":"/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"talk","summary":"Click on the Slides button above to view the built-in slides feature.\n Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using url_slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["GA Cushen"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"5cec0ce6e082b377c504bc66cdf990c5","permalink":"/publication/person-re-identification/","publishdate":"2015-09-01T00:00:00Z","relpermalink":"/publication/person-re-identification/","section":"publication","summary":"Person re-identification is a critical security task for recognizing a person across spatially disjoint sensors. Previous work can be computationally intensive and is mainly based on low-level cues extracted from RGB data and implemented on a PC for a fixed sensor network (such as traditional CCTV). We present a practical and efficient framework for mobile devices (such as smart phones and robots) where high-level semantic soft biometrics are extracted from RGB and depth data. By combining these cues, our approach attempts to provide robustness to noise, illumination, and minor variations in clothing. This mobile approach may be particularly useful for the identification of persons in areas ill-served by fixed sensors or for tasks where the sensor position and direction need to dynamically adapt to a target. Results on the BIWI dataset are preliminary but encouraging. Further evaluation and demonstration of the system will be available on our website.","tags":[],"title":"A Person Re-Identification System For Mobile Devices","type":"publication"},{"authors":["GA Cushen","MS Nixon"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"caae70970030052c8f733b2ca8421a2b","permalink":"/publication/clothing-search/","publishdate":"2013-07-01T00:00:00Z","relpermalink":"/publication/clothing-search/","section":"publication","summary":"We present a mobile visual clothing search system whereby a smart phone user can either choose a social networking photo or take a new photo of a person wearing clothing of interest and search for similar clothing in a retail database. From the query image, the person is detected, clothing is segmented, and clothing features are extracted and quantized. The information is sent from the phone client to a server, where the feature vector of the query image is used to retrieve similar clothing products from online databases. The phone's GPS location is used to re-rank results by retail store location. State of the art work focuses primarily on the recognition of a diverse range of clothing offline and pays little attention to practical applications. Evaluated on a challenging dataset, the system is relatively fast and achieves promising results.","tags":[],"title":"Mobile visual clothing search","type":"publication"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"},{"authors":null,"categories":["R"],"content":" Welcome I’m Tiana Bell! I’ve recently completed the Chromebook Data Science Course Set on Leanpub. I’m interested in working data and answering interesting questions.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"10065deaa3098b0da91b78b48d0efc71","permalink":"/post/2015-07-23-r-rmarkdown/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/2015-07-23-r-rmarkdown/","section":"post","summary":" Welcome I’m Tiana Bell! I’ve recently completed the Chromebook Data Science Course Set on Leanpub. I’m interested in working data and answering interesting questions.\n ","tags":["Welcome"],"title":"Welcome!","type":"post"}]